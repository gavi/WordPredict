filter(data,Shrt_Desc %like% '% heese %')
?filter
library(dplyr)
filter(data,Shrt_Desc %like% '% heese %')
shiny::runApp('App1')
filter(data,grepl('cheese',Shrt_Desc))
shiny::runApp('App1')
shiny::runApp('App1')
data[grepl("heese",Shrt_Desc),]
data[grepl("heese","Shrt_Desc"),]
data[,grepl("heese","Shrt_Desc"),]
data[grepl("heese",data[["Shrt_Desc"]]),]
data[grepl("cheese",data[["Shrt_Desc"]]),]
data[grepl("cheese",data[["Shrt_Desc"]]),]
?grepl
data[["Shrt_Desc"]]
grepl("CHEESE",data[["Shrt_Desc"]])
filter(data,Shrt_Desc like "%CHEESE%")
filter(data,Shrt_Desc %like% "%CHEESE%")
filter(data,Shrt_Desc %like% "CHEESE")
filter(data,Shrt_Desc %like% "CHEESE")
filter(data,lower(Shrt_Desc) %like% "CHEESE")
?lowe
?lower
??lowe
??lower
filter(data,tolower(Shrt_Desc) %like% tolower("CHEESE"))
shiny::runApp('App1')
shiny::runApp('App1')
shiny::runApp('NutrientExplorer')
shiny::runApp('NutrientExplorer')
library(dplyr)
shiny::runApp('NutrientExplorer')
shiny::runApp('NutrientExplorer')
?grepl
shiny::runApp('NutrientExplorer')
library(shinyapps)
deployApp("NutrientExplorer")
install.packages("BH")
deployApp("NutrientExplorer")
shiny::runApp('NutrientExplorer')
deployApp("NutrientExplorer")
shiny::runApp('NutrientExplorer')
deployApp("NutrientExplorer")
library(shiny)
deployApp("NutrientExplorer")
library(shinyapp)
library(shinyapps)
deployApp("NutrientExplorer")
deployApp("App1")
shiny::runApp('NutrientExplorer')
install.packages("Rcpp")
install.packages("Rcpp")
deployApp("NutrientExplorer")
library("shinyapps")
deployApp("NutrientExplorer")
shiny::runApp('NutrientExplorer')
shiny::runApp('NutrientExplorer')
shiny::runApp('NutrientExplorer')
shiny::runApp('NutrientExplorer')
shiny::runApp('NutrientExplorer')
shiny::runApp('NutrientExplorer')
shiny::runApp('NutrientExplorer')
shiny::runApp('NutrientExplorer')
shiny::runApp('NutrientExplorer')
list(a)
list("a","b")
str(list("a","b"))
unlist(list("a","b"))
shiny::runApp('NutrientExplorer')
shiny::runApp('NutrientExplorer')
shiny::runApp('NutrientExplorer')
shiny::runApp('NutrientExplorer')
shiny::runApp('NutrientExplorer')
shiny::runApp('NutrientExplorer')
data<-c("a","b","c")
data.frame(a=1,b=2,c=3)
d<-data.frame(a=1,b=2,c=3)
d
colnames(d)
for col in colnames(d){}
foreach col in colnames(d)
foreach(col in colnames){}
foreach(col:colnames(data)){}
?foreach
?for
''
;
for(col in colnames(d)){}
for(col in colnames(d)){cat(col)}
for(col in colnames(d)){cat(sprinf("%s:%s",col,d[col]))}
for(col in colnames(d)){cat(sprintf("%s:%s",col,d[col]))}
shiny::runApp('NutrientExplorer')
shiny::runApp('NutrientExplorer')
shiny::runApp('NutrientExplorer')
shiny::runApp('NutrientExplorer')
shiny::runApp('NutrientExplorer')
shiny::runApp('NutrientExplorer')
shiny::runApp('NutrientExplorer')
shinyapps::terminateApp("NutrientExplorer")
shinyapps::terminateApp("NutrientExplorer")
shiny::runApp('NutrientExplorer')
shiny::runApp('NutrientExplorer')
shinyapps::terminateApp("NutrientExplorer")
shiny::runApp('NutrientExplorer')
shiny::runApp('NutrientExplorer')
The UI is intuitive. See below explaining the various sections
shiny::runApp('NutrientExplorer')
shiny::runApp('NutrientExplorer')
getMethod
?getMethod
?getS3method
getS3method("mean")
showMethods("mean")
?model
?show
?colSums
?lm
shiny::runApp('NutrientExplorer')
shiny::runApp('NutrientExplorer')
library(datasets)
data(iris)
iris
View(iris)
library(ggplot2)
qplot(iris,sepal.width,sepal.height)
qplot(data=iris,sepal.width,sepal.height)
colnames(iris)
qplot(data=iris,Sepal.Width,Sepal.Height)
?qplot
qplot(Sepal.Width,Sepal.Height,data=iris)
qplot(Sepal.Width,Sepal.Length,data=iris)
library(caret)
?createDataPartition
inTrain<-createDataPartition(iris$Species,list=F)
inTrain
test<-iris[-inTrain]
training<-iris[inTrain]
training<-iris[inTrain,]
test<-iris[-inTrain,]
training<-iris[inTrain,]
?createDataPartition
inTrain<-createDataPartition(iris$Species,p=0.8,list=F)
training<-iris[inTrain,]
test<-iris[-inTrain,]
View(iris)
?trin
?train
train(Species~.,data=training)
model<-train(Species~.,method=rpart,data=training)
library(rpart)
model<-train(Species~.,method=rpart,data=training)
model$finalModel
model<-train(Species~.,method=rpart,data=training)
model<-train(Species~.,method="rpart",data=training)
model<-train(Species~.,method="rpart",data=training)
test
testing<-iris[-inTrain,]
model$finalModel
library(lattice)
rpart.plot
library(rpart.plot)
library(rattle)
fancyRpartPlot(model$finalModel)
View(iris)
predict(model,data=testing)
?predict
predict(model)
predict(model,data=testing)
precit(model,newdata=testing)
predcit(model,newdata=testing)
predict(model,newdata=testing)
testing$Species
output<-predict(model,newdata=testing)
output==testing$Species
sum(output==testing$Species)
sum(output==testing$Species)/numrows(tesing)*100
sum(output==testing$Species)/nrows(tesing)*100
nrows
lenght
length
length(testing)
nrow(testing)
sum(output==testing$Species)/nrow(testing)*100
data(iris)
iris
library(caret)
colnames(iris)
inTrain<-createDataPartition(y=iris$Species,p=1,list=F)
inTrain
inTrain<-createDataPartition(y=iris$Species,p=.7,list=F)
inTrain
rownum
rowcount
nrow(iris)
training<-iris[inTrain,]
testing<-iris[inTrain,]
nrow(training)
model<-train(Species~.,data=training,method="rf")
model$finalModel
fancyRpartPlot(model$finalModel)
library(rattle)
fancyRpartPlot(model$finalModel)
model$finalModel
model<-train(Species~.,data=training,method="rpart")
model$finalModel
fancyRpartPlot(model$finalModel)
output<-predict(model,testing)
output
testing
testing<-iris[-inTrain,]
nrow(testing)
output<-predict(model,testing)
nrow(testing)
output
confusionMatrix(output,testing$Species)
model<-train(Species~.,data=training,method="rf")
output<-predict(model,testing)
confusionMatrix(output,testing$Species)
View(d)
setwd('~/work/R/capstone')
ugram<-table.read('ugram.txt')
ugram<-read.table('ugram.txt')
ugram
ugram<-read.csv('ugram.csv')
View(ugram)
bgram<-read.csv('bgram.csv')
tgram<-read.csv('tgram.csv')
View(tgram)
c("i","love","you")
paste(c("i","love","you"),"")
paste(c("i","love","you")," ")
paste(c("i","love","you"),collapse=" ")
interpolation_probability<-function(items){
if(length(items)==3){
tgram[Var1==paste(items,collapse=" "),]$Freq/length(tgram)
}
}
interpolation_probability(c("i","love","you"))
interpolation_probability<-function(items){
if(length(items)==3){
tgram[tgram$Var1==paste(items,collapse=" "),]$Freq/length(tgram)
}
}
interpolation_probability(c("i","love","you"))
tgram$Var1=="i love you"
tgram[tgram$Var1=="i love you",]
tgram$Var1
tgram[tgram$Var1=="a bucket list",]
tgram[tgram$Var1=="a a a",]
head(tgram)
sample<-head(tgram)
sample$Var1
sample<-head(tgram)=="a a a"
sample
sample$Var1
sample<-head(tgram)=="a a a"
predict<-function(str){
output<-predict4(str)
if(length(output)==0){
output<-predict3(str)
if(length(output)==0){
output<-predict2(str)
}
}
data.frame(suggestions=output)
}
library(tm)
library(stringi)
#Main Tokenizer Function
ngram_tokenizer <- function(n = 1L, skip_word_none = TRUE, skip_word_number = FALSE,filter_words=c()) {
stopifnot(is.numeric(n), is.finite(n), n > 0)
#' To avoid :: calls
stri_split_boundaries <- stringi::stri_split_boundaries
stri_join <- stringi::stri_join
options <- stringi::stri_opts_brkiter(
type="word", skip_word_none = skip_word_none, skip_word_number = skip_word_number
)
#' Tokenizer
#'
#' @param x character
#' @return character vector with n-grams
function(x) {
stopifnot(is.character(x))
# Split into word tokens
tokens <- unlist(stri_split_boundaries(x, opts_brkiter=options))
# If filter words provided, clean up
#if (length(filter_words)>0){
tokens <- tokens[!tokens %in% filter_words]
#}
len <- length(tokens)
if(all(is.na(tokens)) || len < n) {
# If we didn't detect any words or number of tokens is less than n return empty vector
character(0)
} else {
sapply(
1:max(1, len - n + 1),
function(i) stri_join(tokens[i:min(len, i + n - 1)], collapse = " ")
)
}
}
}
#Function that cleans the data and returns the corpus using tm package
cleanData <- function(content){
doc.vec <- VectorSource(list(content))
doc.corpus <- Corpus(doc.vec)
doc.corpus <- tm_map(doc.corpus, removeNumbers) # remove numbers
doc.corpus <- tm_map(doc.corpus, stripWhitespace) # remove whitespaces
doc.corpus <- tm_map(doc.corpus, content_transformer(tolower)) #lowercase all content
doc.corpus <- tm_map(doc.corpus, removePunctuation) # remove any punctuations
doc.corpus
}
#Function to convert corpus back to Text
corpusToText <- function(corpus){
unlist(sapply(corpus, `[`, "content"))
}
#Simple Splitter used for Input Splitting
splitter<-ngram_tokenizer(n=1)
#qgram_index<-read.csv("qgram_index.csv")
#str(qgram_index)
#saveRDS(qgram_index,"q.rds")
ugram<-readRDS("u.rds")
bgram<-readRDS("b.rds")
tgram<-readRDS("t.rds")
qgram<-readRDS("q.rds")
colnames(ugram)<-c("id","word","freq")
colnames(bgram)<-c("id1","id2","freq")
colnames(tgram)<-c("id1","id2","id3","freq")
colnames(qgram)<-c("id1","id2","id3","id4","freq")
lookup<-function(word){
ugram[ugram$word==word,]$id
}
rlookup<-function(ids){
as.character(sapply(ids,function(x)ugram[ugram$id==x,]$word))
}
get_ids<-function(str){
words<-splitter(corpusToText(cleanData(str)))
ret<-c()
for(word in words){
ret<-c(ret,lookup(word))
}
ret
}
lastN<-function(vec,n){
a<-length(vec)-n+1
b<-length(vec)
ret<-vec[a:b]
ret
}
findInQuadgram<-function(ids){
ids<-lastN(ids,3)
d<-qgram[qgram$id1==ids[1]&qgram$id2==ids[2]&qgram$id3==ids[3],]
if(nrow(d)>0){
ret<-head(d[order(d$freq,decreasing=T),],10)
return(ret$id4)
}
}
findInTrigram<-function(ids){
ids<-lastN(ids,2)
d<-tgram[tgram$id1==ids[1]&tgram$id2==ids[2],]
if(nrow(d)>0){
ret<-head(d[order(d$freq,decreasing=T),],10)
return(ret$id3)
}
}
findInBigram<-function(ids){
ids<-lastN(ids,1)
d<-bgram[bgram$id1==ids[1],]
if(nrow(d)>0){
ret<-head(d[order(d$freq,decreasing=T),],10)
return(ret$id2)
}
}
find_matches<-function(ids){
if(length(ids)>=3){
#Get Last 3
}
else{
#Try trgram
else{
}
}
}
}
}
predict2<-function(str){
rlookup(findInBigram(get_ids(str)))
}
predict3<-function(str){
rlookup(findInTrigram(get_ids(str)))
}
predict4<-function(str){
rlookup(findInQuadgram(get_ids(str)))
}
predict<-function(str){
output<-predict4(str)
if(length(output)==0){
output<-predict3(str)
if(length(output)==0){
output<-predict2(str)
}
}
data.frame(suggestions=output)
}
predict("I Love")
predict2("I Love")
predict3("I Love")
predict4("I Love")
LastN(c(1,2),3)
lastN(c(1,2),3)
findInQuadgram<-function(ids){
ids<-lastN(ids,3)
if(lenght(ids)==3){
d<-qgram[qgram$id1==ids[1]&qgram$id2==ids[2]&qgram$id3==ids[3],]
if(nrow(d)>0){
ret<-head(d[order(d$freq,decreasing=T),],10)
return(ret$id4)
}
}
}
predict4("I Love")
findInQuadgram<-function(ids){
ids<-lastN(ids,3)
if(length(ids)==3){
d<-qgram[qgram$id1==ids[1]&qgram$id2==ids[2]&qgram$id3==ids[3],]
if(nrow(d)>0){
ret<-head(d[order(d$freq,decreasing=T),],10)
return(ret$id4)
}
}
}
predict4("I Love")
findInQuadgram<-function(ids){
ids<-lastN(ids,3)
if(length(ids)==3){
d<-qgram[qgram$id1==ids[1]&qgram$id2==ids[2]&qgram$id3==ids[3],]
if(nrow(d)>0){
ret<-head(d[order(d$freq,decreasing=T),],10)
return(ret$id4)
}
}
}
findInTrigram<-function(ids){
ids<-lastN(ids,2)
if(length(ids)==2){
d<-tgram[tgram$id1==ids[1]&tgram$id2==ids[2],]
if(nrow(d)>0){
ret<-head(d[order(d$freq,decreasing=T),],10)
return(ret$id3)
}
}
}
findInBigram<-function(ids){
ids<-lastN(ids,1)
if(length(ids)==1){
d<-bgram[bgram$id1==ids[1],]
if(nrow(d)>0){
ret<-head(d[order(d$freq,decreasing=T),],10)
return(ret$id2)
}
}
}
predict<-function(str){
output<-predict4(str)
if(length(output)==0){
output<-predict3(str)
if(length(output)==0){
output<-predict2(str)
}
}
data.frame(suggestions=output)
}
predict4("I Love")
predict("I Love")
predict("I")
lastN(c(1),3)
lastN<-function(vec,n){
if(length(vec)<n){
return(vec)
}
a<-length(vec)-n+1
b<-length(vec)
ret<-vec[a:b]
ret
}
predict("I")
predict("I am")
predict("I am not")
predict("I am not a")
predict("I am not a fan")
predict("I am not a fan of")
predict("I am not a fan of the")
predict("I am not a fan of the show")
shiny::runApp('wordpredict')
setwd("~/work/R/capstone/wordpredict")
deployApp()
library(shinyapp)
library(shinyapps)
deployApp()
install.packages("shinyapps")
install.packages("shinyapps")
install.packages("shinyapp")
install.packages("shiny")
install.packages("shinyapps")
library("shinyapps", lib.loc="/Library/Frameworks/R.framework/Versions/3.1/Resources/library")
install.packages("shiny")
deployApp()
remove.packages("shinyapps")
install.packages("shiny")
install.packages("shinyapps")
install.packages("shinyapps")
install.packages("shinyapps")
